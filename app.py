import streamlit as st
import pandas as pd
from bs4 import BeautifulSoup
import re
from streamlit_gsheets import GSheetsConnection
from datetime import datetime

# í˜ì´ì§€ ì„¤ì •
st.set_page_config(page_title="ì‚¼ì²œë¦¬ í™ë³´íŒ€ í˜„í™©íŒ", layout="wide")
conn = st.connection("gsheets", type=GSheetsConnection)
SHEET_NAME = "2026ë…„"

st.title("ğŸ¢ ë³´ë„ìë£Œ ê²Œì¬ í˜„í™© ëˆ„ì  ê´€ë¦¬")

# ì…ë ¥ë¶€
col1, col2 = st.columns([1, 2])
with col1:
    doc_date = st.date_input("ë°°í¬ ë‚ ì§œ", datetime.now())
    doc_title = st.text_input("ë³´ë„ìë£Œ ì œëª©")
with col2:
    raw_html = st.text_area("HTML ì†ŒìŠ¤ ë¶™ì—¬ë„£ê¸°", height=200)

if st.button("ğŸš€ í˜„í™©íŒ ëˆ„ì  ì—…ë°ì´íŠ¸"):
    if not doc_title or not raw_html:
        st.warning("ë‚´ìš©ì„ ëª¨ë‘ ì…ë ¥í•´ì£¼ì„¸ìš”.")
    else:
        try:
            with st.spinner("ì—…ë°ì´íŠ¸ ì¤‘..."):
                # header=Noneìœ¼ë¡œ ì½ì–´ì™€ì„œ ì‹œíŠ¸ì˜ 1í–‰(index 0)ë¶€í„° ì¸ì‹í•¨
                df = conn.read(worksheet=SHEET_NAME, header=None).fillna("")
                
                # HTML ë¶„ì„
                soup = BeautifulSoup(raw_html, 'html.parser')
                found_media = {}
                for a_tag in soup.find_all('a', href=True):
                    url = a_tag['href']
                    span = a_tag.find_next_sibling('span')
                    if span:
                        # (ë§¤ì²´ëª… 2026/01/23) íŒ¨í„´ì—ì„œ ë§¤ì²´ëª… ì¶”ì¶œ
                        m = re.search(r'\((.*?) \d{4}', span.get_text())
                        if m: found_media[m.group(1).strip()] = url

                # ìƒˆ ê²°ê³¼ ì—´ ìƒì„± (ì‹œíŠ¸ ì „ì²´ ê¸¸ì´ì— ë§ì¶¤)
                new_col = [""] * len(df)
                
                # [ì¢Œí‘œ ê³ ì •] index 0=1í–‰, 1=2í–‰, 2=3í–‰
                if len(new_col) >= 3:
                    new_col[0] = f"Log_{datetime.now().strftime('%H%M%S')}" # 1í–‰
                    new_col[1] = doc_date.strftime('%m/%d')                # 2í–‰
                    new_col[2] = doc_title                                 # 3í–‰

                match_count = 0
                # index 3(4í–‰)ë¶€í„° ë§¤ì²´ëª… ë¹„êµ ì‹œì‘
                for i in range(len(df)):
                    if i < 3: continue 
                    
                    sheet_media = str(df.iloc[i, 0]).strip()
                    # ìœ íš¨í•œ ë§¤ì²´ëª…ì´ ìˆëŠ” ê²½ìš°ë§Œ ì²˜ë¦¬
                    if not sheet_media or sheet_media in ["0", "1", "ë§¤ì²´ëª…"]: continue
                    
                    # ê´„í˜¸ ì œê±° í›„ ìˆœìˆ˜ ì´ë¦„ìœ¼ë¡œ ë§¤ì¹­
                    pure_name = re.sub(r'\(.*?\)', '', sheet_media).strip()
                    
                    found_url = None
                    for m_name, url in found_media.items():
                        if pure_name in m_name or m_name in pure_name:
                            found_url = url
                            break
                    
                    if found_url:
                        new_col[i] = f'=HYPERLINK("{found_url}", "âœ…")'
                        match_count += 1
                    else:
                        new_col[i] = "-"

                # ê¸°ì¡´ ë°ì´í„°í”„ë ˆì„ì˜ ê°€ì¥ ì˜¤ë¥¸ìª½ì— ìƒˆ ì—´ ì¶”ê°€
                df[f"Col_{datetime.now().strftime('%H%M%S')}"] = new_col
                
                # ì‹œíŠ¸ ì—…ë°ì´íŠ¸
                conn.update(worksheet=SHEET_NAME, data=df)
                st.success(f"âœ… ì—…ë°ì´íŠ¸ ì™„ë£Œ! (ë§¤ì¹­: {match_count}ê±´)")
                st.balloons()
                
        except Exception as e:
            st.error(f"ì˜¤ë¥˜ ë°œìƒ: {e}")
