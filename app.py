import streamlit as st
import pandas as pd
from bs4 import BeautifulSoup
import re

# í˜ì´ì§€ ì„¤ì •
st.set_page_config(page_title="ë§¤ì²´ë³„ ë³´ë„ìë£Œ ê²Œì¬ ì²´í¬", layout="wide")

st.title("ğŸ“Š ë§¤ì²´ë³„ ë³´ë„ìë£Œ ê²Œì¬ ìë™ ì²´í¬ ì‹œìŠ¤í…œ")
st.write("ìš°ë¦¬ ë§¤ì²´ ë¦¬ìŠ¤íŠ¸ì™€ ëª¨ë‹ˆí„°ë§ HTML ì†ŒìŠ¤ë¥¼ ë¹„êµí•˜ì—¬ ê²Œì¬ ì—¬ë¶€ë¥¼ í™•ì¸í•©ë‹ˆë‹¤.")

# ì¢Œìš° í™”ë©´ ë¶„í• 
col1, col2 = st.columns([1, 2])

with col1:
    st.subheader("1. ìš°ë¦¬ ë§¤ì²´ ë¦¬ìŠ¤íŠ¸")
    # íŒ€ì›ë“¤ì´ í‰ì†Œ ê´€ë¦¬í•˜ëŠ” ë§¤ì²´ëª…ì„ í•œ ì¤„ì— í•˜ë‚˜ì”© ì…ë ¥
    media_input = st.text_area("ì²´í¬í•  ë§¤ì²´ëª…ë“¤ì„ ì…ë ¥í•˜ì„¸ìš”. (í•œ ì¤„ì— í•˜ë‚˜ì”©)", 
                              height=400,
                              value="ê°€ìŠ¤ì‹ ë¬¸\nì‹œì‚¬ìºìŠ¤íŠ¸\nì´íˆ¬ë‰´ìŠ¤\nì¡°ì„ ì¼ë³´\në§¤ì¼ê²½ì œ")
    target_media_list = [m.strip() for m in media_input.split('\n') if m.strip()]

with col2:
    st.subheader("2. ëª¨ë‹ˆí„°ë§ HTML ì†ŒìŠ¤")
    raw_html = st.text_area("ëª¨ë‹ˆí„°ë§ ë©”ì¼ì˜ HTML ì†ŒìŠ¤ë¥¼ ë¶™ì—¬ë„£ìœ¼ì„¸ìš”.", height=400)

if st.button("ğŸ” ê²Œì¬ ì—¬ë¶€ ìë™ ì²´í¬ ì‹œì‘"):
    if not target_media_list:
        st.warning("ì™¼ìª½ì— ë§¤ì²´ ë¦¬ìŠ¤íŠ¸ë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”.")
    elif not raw_html:
        st.warning("ì˜¤ë¥¸ìª½ì— ë¶„ì„í•  HTML ì†ŒìŠ¤ë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”.")
    else:
        # 1. HTML ì†ŒìŠ¤ì—ì„œ ê¸°ì‚¬ ì •ë³´ ì¶”ì¶œ
        soup = BeautifulSoup(raw_html, 'html.parser')
        # <td style="...padding-left:20px;"> íƒœê·¸ ì•ˆì— ê¸°ì‚¬ ì •ë³´ê°€ ìˆëŠ” êµ¬ì¡°ë¥¼ í™œìš©
        rows = soup.find_all('td', style=lambda x: x and 'padding-left:20px' in x)
        
        found_articles = {} # {ë§¤ì²´ëª…: ê¸°ì‚¬ì œëª©}
        
        for row in rows:
            link_tag = row.find('a', href=True)
            media_info = row.find('span')
            
            if link_tag and media_info:
                title = link_tag.get_text(strip=True)
                media_text = media_info.get_text(strip=True)
                # ê´„í˜¸ ì•ˆì˜ ë§¤ì²´ëª… ì¶”ì¶œ
                match = re.search(r'\((.*?) \d{4}', media_text)
                if match:
                    extracted_media = match.group(1).strip()
                    found_articles[extracted_media] = title

        # 2. ìš°ë¦¬ ë§¤ì²´ ë¦¬ìŠ¤íŠ¸ì™€ ë¹„êµí•˜ì—¬ ê²°ê³¼ í…Œì´ë¸” ìƒì„±
        check_results = []
        for media in target_media_list:
            if media in found_articles:
                check_results.append({
                    "ë§¤ì²´ëª…": media,
                    "ê²Œì¬ ì—¬ë¶€": "âœ… ê²Œì¬ ì™„ë£Œ",
                    "ê¸°ì‚¬ ì œëª©": found_articles[media]
                })
            else:
                check_results.append({
                    "ë§¤ì²´ëª…": media,
                    "ê²Œì¬ ì—¬ë¶€": "âŒ ë¯¸ê²Œì¬",
                    "ê¸°ì‚¬ ì œëª©": "-"
                })

        # 3. ê²°ê³¼ ì¶œë ¥
        st.subheader("ğŸ“ ìµœì¢… ê²Œì¬ í˜„í™© ë¦¬í¬íŠ¸")
        df_final = pd.DataFrame(check_results)
        
        # í‘œ í˜•ì‹ìœ¼ë¡œ ë³´ì—¬ì£¼ê¸°
        st.dataframe(df_final, use_container_width=True, hide_index=True)
        
        # í†µê³„ ìš”ì•½
        total = len(target_media_list)
        success = sum(1 for r in check_results if r["ê²Œì¬ ì—¬ë¶€"] == "âœ… ê²Œì¬ ì™„ë£Œ")
        st.info(f"ì´ {total}ê°œ ë§¤ì²´ ì¤‘ {success}ê°œ ë§¤ì²´ ê²Œì¬ í™•ì¸ (ê²Œì¬ìœ¨: {round(success/total*100, 1)}%)")

        # ì—‘ì…€ ë‹¤ìš´ë¡œë“œ ë²„íŠ¼
        csv = df_final.to_csv(index=False).encode('utf-8-sig')
        st.download_button("ê²°ê³¼ ë¦¬í¬íŠ¸ ì €ì¥ (Excel/CSV)", csv, "coverage_check.csv", "text/csv")
